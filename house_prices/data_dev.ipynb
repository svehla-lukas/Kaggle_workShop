{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from data_set/train.csv\n",
      "dimension train data: (1460, 81)\n"
     ]
    }
   ],
   "source": [
    "path_train_data = \"data_set/train.csv\"\n",
    "\n",
    "train_data = utils_io.load_csv_data(path_train_data)\n",
    "# Debugging: Check the type of train_data\n",
    "\n",
    "# Print the loaded data\n",
    "if isinstance(train_data, pd.DataFrame):\n",
    "    print(f\"dimension train data: {train_data.shape}\")\n",
    "else:\n",
    "    print(\"train_data is not a DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "   Id  MSSubClass MSZoning LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL          65     8450   Pave    NA      Reg   \n",
      "1   2          20       RL          80     9600   Pave    NA      Reg   \n",
      "2   3          60       RL          68    11250   Pave    NA      IR1   \n",
      "3   4          70       RL          60     9550   Pave    NA      IR1   \n",
      "4   5          60       RL          84    14260   Pave    NA      IR1   \n",
      "5   6          50       RL          85    14115   Pave    NA      IR1   \n",
      "6   7          20       RL          75    10084   Pave    NA      Reg   \n",
      "7   8          60       RL          NA    10382   Pave    NA      IR1   \n",
      "8   9          50       RM          51     6120   Pave    NA      Reg   \n",
      "9  10         190       RL          50     7420   Pave    NA      Reg   \n",
      "\n",
      "  LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
      "0         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "1         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "2         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "3         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "4         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "5         Lvl    AllPub  ...        0     NA  MnPrv        Shed     700   \n",
      "6         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "7         Lvl    AllPub  ...        0     NA     NA        Shed     350   \n",
      "8         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "9         Lvl    AllPub  ...        0     NA     NA          NA       0   \n",
      "\n",
      "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0      2   2008        WD         Normal     208500  \n",
      "1      5   2007        WD         Normal     181500  \n",
      "2      9   2008        WD         Normal     223500  \n",
      "3      2   2006        WD        Abnorml     140000  \n",
      "4     12   2008        WD         Normal     250000  \n",
      "5     10   2009        WD         Normal     143000  \n",
      "6      8   2007        WD         Normal     307000  \n",
      "7     11   2009        WD         Normal     200000  \n",
      "8      4   2008        WD        Abnorml     129900  \n",
      "9      1   2008        WD         Normal     118000  \n",
      "\n",
      "[10 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "printdata = train_data.iloc[:10]\n",
    "print(train_data.shape)\n",
    "print(printdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460,)\n",
      "(1460, 80)\n"
     ]
    }
   ],
   "source": [
    "# Get target data\n",
    "target_data = train_data[\"SalePrice\"]\n",
    "print(target_data.shape)\n",
    "\n",
    "\n",
    "# Drop the Id and sales price column\n",
    "train_data = train_data.drop(columns=[\"Id\"])\n",
    "# train_data = train_data.drop(columns=[\"SalePrice\"])\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0             60       RL         65.0     8450   PAVE    NA      REG   \n",
      "1             20       RL         80.0     9600   PAVE    NA      REG   \n",
      "2             60       RL         68.0    11250   PAVE    NA      IR1   \n",
      "3             70       RL         60.0     9550   PAVE    NA      IR1   \n",
      "4             60       RL         84.0    14260   PAVE    NA      IR1   \n",
      "...          ...      ...          ...      ...    ...   ...      ...   \n",
      "1455          60       RL         62.0     7917   PAVE    NA      REG   \n",
      "1456          20       RL         85.0    13175   PAVE    NA      REG   \n",
      "1457          70       RL         66.0     9042   PAVE    NA      REG   \n",
      "1458          20       RL         68.0     9717   PAVE    NA      REG   \n",
      "1459          20       RL         75.0     9937   PAVE    NA      REG   \n",
      "\n",
      "     LandContour Utilities LotConfig  ... PoolArea PoolQC  Fence MiscFeature  \\\n",
      "0            LVL    ALLPUB    INSIDE  ...        0     NA     NA          NA   \n",
      "1            LVL    ALLPUB       FR2  ...        0     NA     NA          NA   \n",
      "2            LVL    ALLPUB    INSIDE  ...        0     NA     NA          NA   \n",
      "3            LVL    ALLPUB    CORNER  ...        0     NA     NA          NA   \n",
      "4            LVL    ALLPUB       FR2  ...        0     NA     NA          NA   \n",
      "...          ...       ...       ...  ...      ...    ...    ...         ...   \n",
      "1455         LVL    ALLPUB    INSIDE  ...        0     NA     NA          NA   \n",
      "1456         LVL    ALLPUB    INSIDE  ...        0     NA  MNPRV          NA   \n",
      "1457         LVL    ALLPUB    INSIDE  ...        0     NA  GDPRV        SHED   \n",
      "1458         LVL    ALLPUB    INSIDE  ...        0     NA     NA          NA   \n",
      "1459         LVL    ALLPUB    INSIDE  ...        0     NA     NA          NA   \n",
      "\n",
      "     MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0          0      2    2008        WD         NORMAL     208500  \n",
      "1          0      5    2007        WD         NORMAL     181500  \n",
      "2          0      9    2008        WD         NORMAL     223500  \n",
      "3          0      2    2006        WD        ABNORML     140000  \n",
      "4          0     12    2008        WD         NORMAL     250000  \n",
      "...      ...    ...     ...       ...            ...        ...  \n",
      "1455       0      8    2007        WD         NORMAL     175000  \n",
      "1456       0      2    2010        WD         NORMAL     210000  \n",
      "1457    2500      5    2010        WD         NORMAL     266500  \n",
      "1458       0      4    2010        WD         NORMAL     142125  \n",
      "1459       0      6    2008        WD         NORMAL     147500  \n",
      "\n",
      "[1460 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def auto_clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    for column in df_cleaned.columns:\n",
    "        values = df_cleaned[column].dropna().unique()\n",
    "\n",
    "        can_be_numeric = True\n",
    "        for v in values:\n",
    "            if isinstance(v, str) and v.strip().upper() == \"NA\":\n",
    "                continue\n",
    "            try:\n",
    "                float(v)\n",
    "            except:\n",
    "                can_be_numeric = False\n",
    "                break\n",
    "\n",
    "        if can_be_numeric:\n",
    "            # P≈ôev√©st \"NA\" na np.nan a sloupec na float\n",
    "            df_cleaned[column] = df_cleaned[column].replace(\"NA\", np.nan)\n",
    "            df_cleaned[column] = pd.to_numeric(df_cleaned[column], errors=\"coerce\")\n",
    "        else:\n",
    "            # ponech√°me jako kategorii se stringy\n",
    "            df_cleaned[column] = df_cleaned[column].astype(str).str.strip().str.upper()\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "train_data = auto_clean_columns(train_data)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_features(df: pd.DataFrame, target_column: str):\n",
    "    \"\"\"\n",
    "    X_train = vstupn√≠ featury (normalizovan√©, zak√≥dovan√©, p≈ôipraven√©)\n",
    "    y_train = c√≠lov√° hodnota (ve tv√©m p≈ô√≠padƒõ MSSubClass)\n",
    "    transformer = pipeline pro pou≈æit√≠ na testovac√≠ch datech\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Rozdƒõlen√≠ c√≠lov√© promƒõnn√© a vstup≈Ø\n",
    "    y = df[target_column]\n",
    "    X = df.drop(columns=[target_column])\n",
    "\n",
    "    # 2. Rozdƒõlen√≠ sloupc≈Ø\n",
    "    numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "    # 3. Tvorba transformac√≠\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numerical_cols),\n",
    "            (\n",
    "                \"cat\",\n",
    "                OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "                categorical_cols,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 4. Pipeline (m≈Ø≈æe≈° pou≈æ√≠t i s modely jako XGBoost, NN atd.)\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n",
    "\n",
    "    # 5. Fit + transform vstupn√≠ data\n",
    "    X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "    # 6. Z√≠sk√°n√≠ n√°zv≈Ø nov√Ωch sloupc≈Ø (voliteln√©)\n",
    "    new_columns = []\n",
    "\n",
    "    if numerical_cols:\n",
    "        new_columns += numerical_cols\n",
    "\n",
    "    if categorical_cols:\n",
    "        encoder = pipeline.named_steps[\"preprocessor\"].named_transformers_[\"cat\"]\n",
    "        new_columns += encoder.get_feature_names_out(categorical_cols).tolist()\n",
    "\n",
    "    X_transformed_df = pd.DataFrame(X_transformed, columns=new_columns, index=X.index)\n",
    "\n",
    "    return X_transformed_df, y, pipeline  # X, y, transformer pro testovac√≠ data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train = vstupn√≠ featury (normalizovan√©, zak√≥dovan√©, p≈ôipraven√©)\n",
    "- y_train = c√≠lov√° hodnota (ve tv√©m p≈ô√≠padƒõ MSSubClass)\n",
    "- transformer = pipeline pro pou≈æit√≠ na testovac√≠ch datech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
      "                                                  ['MSSubClass', 'LotFrontage',\n",
      "                                                   'LotArea', 'OverallQual',\n",
      "                                                   'OverallCond', 'YearBuilt',\n",
      "                                                   'YearRemodAdd', 'MasVnrArea',\n",
      "                                                   'BsmtFinSF1', 'BsmtFinSF2',\n",
      "                                                   'BsmtUnfSF', 'TotalBsmtSF',\n",
      "                                                   '1stFlrSF', '2ndFlrSF',\n",
      "                                                   'LowQualFinSF', 'GrLivArea',\n",
      "                                                   'BsmtFullBath',\n",
      "                                                   'BsmtHalfBath', 'FullBath',\n",
      "                                                   'HalfBath', 'Be...\n",
      "                                                   'Alley', 'LotShape',\n",
      "                                                   'LandContour', 'Utilities',\n",
      "                                                   'LotConfig', 'LandSlope',\n",
      "                                                   'Neighborhood', 'Condition1',\n",
      "                                                   'Condition2', 'BldgType',\n",
      "                                                   'HouseStyle', 'RoofStyle',\n",
      "                                                   'RoofMatl', 'Exterior1st',\n",
      "                                                   'Exterior2nd', 'MasVnrType',\n",
      "                                                   'ExterQual', 'ExterCond',\n",
      "                                                   'Foundation', 'BsmtQual',\n",
      "                                                   'BsmtCond', 'BsmtExposure',\n",
      "                                                   'BsmtFinType1',\n",
      "                                                   'BsmtFinType2', 'Heating',\n",
      "                                                   'HeatingQC', 'CentralAir',\n",
      "                                                   'Electrical', ...])]))])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, transformer = prepare_features(train_data, \"SalePrice\")\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare test data with transformer from test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from data_set/test.csv\n",
      "dimension train data: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "path_test_data = \"data_set/test.csv\"\n",
    "\n",
    "test_data = utils_io.load_csv_data(path_test_data)\n",
    "# Debugging: Check the type of test_data\n",
    "\n",
    "# Print the loaded data\n",
    "if isinstance(test_data, pd.DataFrame):\n",
    "    print(f\"dimension train data: {test_data.shape}\")\n",
    "else:\n",
    "    print(\"test_data is not a DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 79)\n"
     ]
    }
   ],
   "source": [
    "test_data = test_data.drop(columns=[\"Id\"])\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 79)\n"
     ]
    }
   ],
   "source": [
    "test_data = auto_clean_columns(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konverze zpƒõt z pandas \"string\" typu na obyƒçejn√Ω Python string (pro OneHotEncoder)\n",
    "for col in test_data.select_dtypes(include=[\"string\"]).columns:\n",
    "    test_data[col] = test_data[col].astype(\"object\")\n",
    "\n",
    "# Potom transformace\n",
    "X_test = transformer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace NA as np.nan in numbers / replace NA witrh \"NA\" oin string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def auto_clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    for column in df_cleaned.columns:\n",
    "        values = df_cleaned[column].dropna().unique()\n",
    "\n",
    "        can_be_numeric = True\n",
    "        for v in values:\n",
    "            if isinstance(v, str) and v.strip().upper() == \"NA\":\n",
    "                continue\n",
    "            try:\n",
    "                float(v)\n",
    "            except:\n",
    "                can_be_numeric = False\n",
    "                break\n",
    "\n",
    "        if can_be_numeric:\n",
    "            # P≈ôev√©st \"NA\" na np.nan a sloupec na float\n",
    "            df_cleaned[column] = df_cleaned[column].replace(\"NA\", np.nan)\n",
    "            df_cleaned[column] = pd.to_numeric(df_cleaned[column], errors=\"coerce\")\n",
    "        else:\n",
    "            # ponech√°me jako kategorii se stringy\n",
    "            df_cleaned[column] = df_cleaned[column].astype(str).str.strip().str.upper()\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "train_data = auto_clean_columns(train_data)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate .json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def auto_generate_description_dict(train_data: pd.DataFrame) -> dict:\n",
    "    result = {}\n",
    "\n",
    "    for column in train_data.columns:\n",
    "        result[column] = {\"description\": \"\", \"items\": {}}\n",
    "\n",
    "        id_counter = 0\n",
    "        seen = {}\n",
    "\n",
    "        for value in train_data[column]:\n",
    "            if isinstance(value, str):\n",
    "                # Add NA for first every column\n",
    "                if \"NA\" not in seen:\n",
    "                    result[column][\"items\"][\"NA\"] = {\"id\": id_counter}\n",
    "                    seen[\"NA\"] = True\n",
    "                    id_counter += 1\n",
    "\n",
    "                key = value.strip().upper()\n",
    "                if key not in seen:\n",
    "                    result[column][\"items\"][key] = {\"id\": id_counter}\n",
    "                    seen[key] = True\n",
    "                    id_counter += 1\n",
    "            # else:\n",
    "            # print(column)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "#  Create descriptions from train data (DO NOT DO IT FROM TEST DATA)\n",
    "description_dictionary = auto_generate_description_dict(train_data)\n",
    "print(description_dictionary)\n",
    "\n",
    "with open(\"data_set/generated_description.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(description_dictionary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ JSON dictionary generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.iloc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace string with values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "txt_path = \"data_set/data_description.txt\"\n",
    "json_path = \"data_set/generated_description.json\"\n",
    "\n",
    "\n",
    "# Pou≈æit√≠\n",
    "# utils_io.parse_description_txt_to_json(txt_path, json_path)\n",
    "\n",
    "generated_description = json.load(open(json_path))\n",
    "\n",
    "\n",
    "for column in train_data.columns:\n",
    "    for index, value in train_data[column].items():\n",
    "        # if value == \"C (all)\":\n",
    "        #     print(column, index, value)\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                train_data.at[index, column] = generated_description[column][\"items\"][\n",
    "                    value.strip().upper()\n",
    "                ][\"id\"]\n",
    "            except:\n",
    "                print(column, index, value, isinstance(value, str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobr√Ω den,\n",
    "Pros√≠m bylo by mo≈æn√© zamluvit chatu v osazen√≠\n",
    "9 dospƒõl√≠ch 2 dƒõti do 10 let, 2 dƒõti do 2 let,\n",
    "v oznaƒçen√©m term√≠nu 17-20.4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.iloc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc mean and std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "# print(mean)\n",
    "std = train_data.std(axis=0)\n",
    "# print(std)\n",
    "\n",
    "train_data = train_data - mean\n",
    "train_data = train_data / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 7\n",
    "print(max(train_data.iloc[:, column]))\n",
    "print(min(train_data.iloc[:, column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NN neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755000\n",
      "34900\n",
      "180921.19589041095\n",
      "720100\n"
     ]
    }
   ],
   "source": [
    "print(max(target_data))\n",
    "print(min(target_data))\n",
    "print(np.mean(target_data))\n",
    "print(max(target_data) - min(target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype(\"float32\")\n",
    "target_data = target_data.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, y_train, transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n",
      "processing fold # 0\n",
      "(1460,)\n",
      "(365, 80)\n",
      "(365,)\n",
      "(1095, 80)\n",
      "(1095,)\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "print(num_val_samples)\n",
    "print(f\"processing fold # {i}\")\n",
    "print(target_data.shape)\n",
    "val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples]\n",
    "val_targets = target_data[i * num_val_samples : (i + 1) * num_val_samples]\n",
    "print(val_data.shape)\n",
    "print(val_targets.shape)\n",
    "\n",
    "partial_train_data = pd.concat(\n",
    "    [train_data[: i * num_val_samples], train_data[(i + 1) * num_val_samples :]]\n",
    ")\n",
    "partial_train_targets = pd.concat(\n",
    "    [target_data[: i * num_val_samples], target_data[(i + 1) * num_val_samples :]]\n",
    ")\n",
    "print(partial_train_data.shape)\n",
    "print(partial_train_targets.shape)\n",
    "# print(partial_train_data.shape)\n",
    "print(train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prediction = np.full_like(target_data, np.mean(target_data))\n",
    "baseline_mae = np.mean(np.abs(target_data - baseline_prediction))\n",
    "print(baseline_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    input_shape = (79,)  # 79 parameters of house\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))  # regresn√≠ v√Ωstup\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# def build_model():\n",
    "#     model = keras.models.Sequential()\n",
    "#     input_shape = (79,)  # 79 parameters of house\n",
    "#     model.add(\n",
    "#         keras.layers.Dense(units=256, activation=\"relu\"),\n",
    "#     )\n",
    "#     model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "#     model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "#     model.add(keras.layers.Dense(units=1))  # output layer price of house\n",
    "#     model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "#     return model\n",
    "\n",
    "\n",
    "k = 5\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "all_histories = []\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"processing fold # {i}\")\n",
    "    val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples]\n",
    "    val_targets = target_data[i * num_val_samples : (i + 1) * num_val_samples]\n",
    "    partial_train_data = pd.concat(\n",
    "        [train_data[: i * num_val_samples], train_data[(i + 1) * num_val_samples :]]\n",
    "    )\n",
    "    partial_train_targets = pd.concat(\n",
    "        [target_data[: i * num_val_samples], target_data[(i + 1) * num_val_samples :]]\n",
    "    )\n",
    "\n",
    "    model = build_model()\n",
    "    print(\"- üß† Spou≈°t√≠m tr√©nov√°n√≠...\")\n",
    "    history = model.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=64,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    print(f\"- ‚úÖ Fold {i + 1} finished: val_mae = {val_mae:.2f}\")\n",
    "    all_scores.append(val_mae)\n",
    "    all_histories.append(history)\n",
    "\n",
    "print(all_scores)\n",
    "print(f\"üïí Celkov√Ω ƒças tr√©nov√°n√≠: {time.time() - start:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"house_prices_0.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìä V√Ωsledky cross-validace:\")\n",
    "for i, score in enumerate(all_scores):\n",
    "    print(f\"Fold {i + 1}: MAE = {score:.2f}\")\n",
    "\n",
    "average = np.mean(all_scores)\n",
    "print(\"-\" * 50)\n",
    "print(f\"üìà Pr≈Ømƒõrn√° MAE p≈ôes {k} fold≈Ø: {average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_maes = [\n",
    "    [history.history[\"mae\"][epoch] for history in all_histories]\n",
    "    for epoch in range(num_epochs)\n",
    "]\n",
    "average_mae_history = [np.mean(epoch) for epoch in epoch_maes]\n",
    "std_mae_history = [np.std(epoch) for epoch in epoch_maes]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    range(1, num_epochs + 1), average_mae_history, marker=\"o\", label=\"Pr≈Ømƒõrn√° MAE\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    range(1, num_epochs + 1),\n",
    "    np.array(average_mae_history) - np.array(std_mae_history),\n",
    "    np.array(average_mae_history) + np.array(std_mae_history),\n",
    "    alpha=0.2,\n",
    "    label=\"¬± 1 std\",\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Absolute Error (MAE)\")\n",
    "plt.title(\"üìà Pr≈Ømƒõrn√° MAE s rozptylem (v≈°echny foldy)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"GPU available:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    print(\"Running on GPU ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.platform.build_info' has no attribute 'cuda_version_number'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_info \u001b[38;5;28;01mas\u001b[39;00m tf_build_info\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtf_build_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_version_number\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuDNN version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf_build_info\u001b[38;5;241m.\u001b[39mcudnn_version_number)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.platform.build_info' has no attribute 'cuda_version_number'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "\n",
    "print(\"CUDA version:\", tf_build_info.cuda_version_number)\n",
    "print(\"cuDNN version:\", tf_build_info.cudnn_version_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
